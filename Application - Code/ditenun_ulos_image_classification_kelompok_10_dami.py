# -*- coding: utf-8 -*-
"""DiTenun - Ulos Image Classification - Kelompok 10 - DaMi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DksMunCIWABD9_S2EAiF8g3Il92gBJk7

# **`DiTenun - Ulos Image Classification`**

# **`Business Understanding`**

### **`Based on Data`***

Ulos adalah kain tradisional suku Batak yang memiliki berbagai motif sesuai dengan makna budayanya. Motif yang beragam dengan variasi dan pola yang rumit sering kali menyebabkan penggolongan ulos yang cukup rumit. Tugas analitik utama adalah klasifikasi motif ulos. Dengan menggunakan algoritma Convolutional Neural Network (CNN), proyek ini bertujuan untuk memprediksi kelas atau kategori ulos berdasarkan gambar motifnya.

Data yang diperlukan adalah gambar ulos yang memiliki deskripsi tentang jenis ulosnya (misalnya: Ulos Ragidup, Ulos Sibolang, Ulos Sadum, dan lain-lain). Dataset dapat diakses dari Kaggle. Dataset berisi 1.231 gambar motif ulos dengan 6 label yang menunjukkan jenis motif ulos (Pinuncaan, Ragi Hidup, Ragi Hotang, Sadum, Sibolang, Tumtuman). Dataset ini merupakan dataset via public data. Data sudah dibagi untuk train dan test data.

### **`Rencana Pelaksanaan Proyek`***

Ruang lingkup (Berdasarkan Work Breakdown Structure) dari proyek ini adalah sebagai berikut:

1. Persiapan
Pemilihan kasus, yaitu identifikasi masalah klasifikasi motif ulos.
Penentuan Algoritma, yaitu memilih algoritma CNN sebagai metode untuk klasifikasi gambar.

2. Pelaksanaan

- Business Understanding, yaitu mementukan objektif proyek, tujuan proyek, dan rencana proyek.
- Data Understanding, yaitu mengumpulkan data, menelaah data, dan memvalidasi data.
- Data Preparation, yaitu memilah , membersikan, mengkonstruksi, menentukan label, dan mengintegrasikan data.
- Modeling, yaitu membangun skenario pengujian, dan membangun model.
- Model Evaluation, yaitu mengevaluasi hasil pemodelan, dan melakukan review proses pemodelan.
- Deployment, yaitu melakukan deployment model dan membuat laporan akhir proyek.

Timeline yang dibutuhkan untuk melakukan proyek ini adalah sekitar 5 minggu untuk proses pengumpulan dan pelabelan data sampai dengan implementasi (deployment).

# **`Data Understanding`**
"""

!pip install kagglehub

"""### **`Mengumpulkan Data`**"""

import kagglehub

# Download dataset kain ulos
path = kagglehub.dataset_download("fthnaja/kain-ulos")

print("Path to dataset files:", path)

"""**`Jumlah Data`**"""

import os
import pandas as pd
from tabulate import tabulate

# Path dataset yang telah diunduh
dataset_path = '/root/.cache/kagglehub/datasets/fthnaja/kain-ulos/versions/3'

# Fungsi untuk menghitung jumlah gambar di seluruh dataset dan menampilkan sebagai tabel
def count_images_per_category(dataset_path):
    category_counts = {}

    # Menelusuri seluruh file dan folder
    for root, dirs, files in os.walk(dataset_path):
        for file in files:
            if not file.startswith('.') and file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp')):
                # Menentukan kategori berdasarkan nama folder
                category = os.path.basename(root)
                if category not in category_counts:
                    category_counts[category] = 0
                category_counts[category] += 1

    # Mengubah hasil ke dalam bentuk DataFrame untuk tampilan tabel
    category_df = pd.DataFrame(list(category_counts.items()), columns=['Category', 'Image Count'])
    return category_df

# Hitung jumlah gambar per kategori dan tampilkan
category_df = count_images_per_category(dataset_path)

# Menampilkan tabel jumlah gambar per kategori menggunakan tabulate
print(tabulate(category_df, headers='keys', tablefmt='pretty', showindex=False))

# Menampilkan jumlah total gambar
total_images = category_df['Image Count'].sum()
print(f"\nJumlah total gambar dalam dataset: {total_images}")

"""**`Deskripsi Data`**"""

import os
import pandas as pd
from tabulate import tabulate

# Path dataset
dataset_path = '/root/.cache/kagglehub/datasets/fthnaja/kain-ulos/versions/3'

# Fungsi untuk mendapatkan deskripsi data
def describe_dataset(dataset_path):
    file_details = []

    # Menelusuri seluruh file dan folder
    for root, dirs, files in os.walk(dataset_path):
        for file in files:
            if not file.startswith('.') and file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp')):
                # Menentukan kategori berdasarkan nama folder
                category = os.path.basename(root)
                file_format = file.split('.')[-1]  # Mendapatkan ekstensi file
                file_details.append({'Category': category, 'File Name': file, 'File Format': file_format})

    # Mengubah hasil ke dalam bentuk DataFrame
    file_df = pd.DataFrame(file_details)
    return file_df

# Mendapatkan deskripsi dataset
file_df = describe_dataset(dataset_path)

# Menampilkan semua deskripsi dataset menggunakan tabulate
print(tabulate(file_df, headers='keys', tablefmt='pretty', showindex=False))  # Menampilkan semua baris

# Menampilkan jumlah total file yang dianalisis
print(f"\nJumlah total file dalam dataset: {len(file_df)}")

import os
import pandas as pd
from tabulate import tabulate

# Path dataset
dataset_path = '/root/.cache/kagglehub/datasets/fthnaja/kain-ulos/versions/3'

# Fungsi untuk mendapatkan deskripsi data
def describe_dataset(dataset_path):
    file_details = []

    # Menelusuri seluruh file dan folder
    for root, dirs, files in os.walk(dataset_path):
        for file in files:
            if not file.startswith('.') and file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp')):
                # Menentukan kategori berdasarkan nama folder
                category = os.path.basename(root)
                file_format = file.split('.')[-1]  # Mendapatkan ekstensi file
                file_details.append({'Category': category, 'File Name': file, 'File Format': file_format})

    # Mengubah hasil ke dalam bentuk DataFrame
    file_df = pd.DataFrame(file_details)
    return file_df

# Mendapatkan deskripsi dataset
file_df = describe_dataset(dataset_path)

# Menampilkan data dengan maksimal 5 baris per kategori
for category in file_df['Category'].unique():
    category_df = file_df[file_df['Category'] == category].head(2)
    print(f"## {category}")
    print(tabulate(category_df, headers='keys', tablefmt='pretty', showindex=False))
    print()

# Menampilkan jumlah total file yang dianalisis
print(f"\nJumlah total file dalam dataset: {len(file_df)}")

"""### **`Menelaah Data`**

**`Karakteristik data`**
"""

import os
import pandas as pd
from tabulate import tabulate

# Path dataset
dataset_path = '/root/.cache/kagglehub/datasets/fthnaja/kain-ulos/versions/3'

# Fungsi untuk mendapatkan deskripsi data lengkap dengan karakteristik
def describe_dataset(dataset_path):
    file_details = []

    # Menelusuri seluruh file dan folder
    for root, dirs, files in os.walk(dataset_path):
        for file in files:
            if not file.startswith('.') and file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp')):
                # Menentukan kategori berdasarkan nama folder
                category = os.path.basename(root)
                file_format = file.split('.')[-1]  # Mendapatkan ekstensi file
                file_path = os.path.join(root, file)  # Mendapatkan path file lengkap

                # Mendapatkan karakteristik file: ukuran dan tanggal terakhir dimodifikasi
                file_size = os.path.getsize(file_path)  # Ukuran file dalam byte
                last_modified = os.path.getmtime(file_path)  # Timestamp terakhir dimodifikasi

                # Menyimpan informasi dalam bentuk dictionary
                file_details.append({
                    'Category': category,
                    'File Name': file,
                    'File Format': file_format,
                    'File Size (bytes)': file_size,
                    'Last Modified': last_modified
                })

    # Mengubah hasil ke dalam bentuk DataFrame
    file_df = pd.DataFrame(file_details)
    return file_df

# Mendapatkan deskripsi dataset
file_df = describe_dataset(dataset_path)

# Menampilkan semua deskripsi dataset menggunakan tabulate
print(tabulate(file_df, headers='keys', tablefmt='pretty', showindex=False))  # Menampilkan semua baris

# Menampilkan jumlah total file yang dianalisis
print(f"\nJumlah total file dalam dataset: {len(file_df)}")

import os
import pandas as pd
from tabulate import tabulate

# Path dataset
dataset_path = '/root/.cache/kagglehub/datasets/fthnaja/kain-ulos/versions/3'

# Fungsi untuk mendapatkan deskripsi data lengkap dengan karakteristik
def describe_dataset(dataset_path):
    file_details = []

    # Menelusuri seluruh file dan folder
    for root, dirs, files in os.walk(dataset_path):
        for file in files:
            if not file.startswith('.') and file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp')):
                # Menentukan kategori berdasarkan nama folder
                category = os.path.basename(root)
                file_format = file.split('.')[-1]  # Mendapatkan ekstensi file
                file_path = os.path.join(root, file)  # Mendapatkan path file lengkap

                # Mendapatkan karakteristik file: ukuran dan tanggal terakhir dimodifikasi
                file_size = os.path.getsize(file_path)  # Ukuran file dalam byte
                last_modified = os.path.getmtime(file_path)  # Timestamp terakhir dimodifikasi

                # Menyimpan informasi dalam bentuk dictionary
                file_details.append({
                    'Category': category,
                    'File Name': file,
                    'File Format': file_format,
                    'File Size (bytes)': file_size,
                    'Last Modified': last_modified
                })

    # Mengubah hasil ke dalam bentuk DataFrame
    file_df = pd.DataFrame(file_details)
    return file_df

# Mendapatkan deskripsi dataset
file_df = describe_dataset(dataset_path)

# Menampilkan deskripsi dataset dengan menampilkan 2 file per label
for label in file_df['Category'].unique():
    label_df = file_df[file_df['Category'] == label].head(2)  # Mengambil 2 file pertama dari setiap label
    print(f"\n{label}")
    print(tabulate(label_df, headers='keys', tablefmt='pretty', showindex=False))

# Menampilkan jumlah total file yang dianalisis
print(f"\nJumlah total file dalam dataset: {len(file_df)}")

"""**`Keterkaitan data`**"""

import os
import pandas as pd
import numpy as np
from scipy.stats import chi2_contingency, kendalltau, f_oneway
from sklearn.feature_selection import mutual_info_classif
import seaborn as sns
import matplotlib.pyplot as plt
from tabulate import tabulate

# Path dataset
dataset_path = '/root/.cache/kagglehub/datasets/fthnaja/kain-ulos/versions/3'

# Fungsi untuk mendapatkan deskripsi data lengkap dengan karakteristik
def describe_dataset(dataset_path):
    file_details = []

    # Menelusuri seluruh file dan folder
    for root, dirs, files in os.walk(dataset_path):
        for file in files:
            if not file.startswith('.') and file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp')):
                # Menentukan kategori berdasarkan nama folder
                category = os.path.basename(root)
                file_format = file.split('.')[-1]  # Mendapatkan ekstensi file
                file_path = os.path.join(root, file)  # Mendapatkan path file lengkap

                # Mendapatkan karakteristik file: ukuran dan tanggal terakhir dimodifikasi
                file_size = os.path.getsize(file_path)  # Ukuran file dalam byte
                last_modified = os.path.getmtime(file_path)  # Timestamp terakhir dimodifikasi

                # Menyimpan informasi dalam bentuk dictionary
                file_details.append({
                    'Category': category,
                    'File Name': file,
                    'File Format': file_format,
                    'File Size (bytes)': file_size,
                    'Last Modified': last_modified
                })

    # Mengubah hasil ke dalam bentuk DataFrame
    file_df = pd.DataFrame(file_details)
    return file_df

# Mendapatkan deskripsi dataset
file_df = describe_dataset(dataset_path)

# Uji ANOVA: Uji perbedaan ukuran file antar kategori
categories = file_df['Category'].unique()
file_sizes_by_category = [file_df[file_df['Category'] == category]['File Size (bytes)'] for category in categories]

# Melakukan uji ANOVA
f_stat, p_value = f_oneway(*file_sizes_by_category)

# Menampilkan hasil uji ANOVA
print(f"\nANOVA Test Results:")
print(f"F-statistic: {f_stat}, p-value: {p_value}")

if p_value < 0.05:
    print("Ada perbedaan signifikan dalam ukuran file antar kategori.")
else:
    print("Tidak ada perbedaan signifikan dalam ukuran file antar kategori.")

# Kendall's Tau: Mengukur asosiasi antara dua variabel ordinal (misalnya ukuran file dan kategori numerik)
# Mengonversi kategori menjadi angka untuk analisis
category_mapping = {category: idx for idx, category in enumerate(file_df['Category'].unique())}
file_df['Category Numeric'] = file_df['Category'].map(category_mapping)

kendall_corr, kendall_p_value = kendalltau(file_df['File Size (bytes)'], file_df['Category Numeric'])

# Menampilkan hasil Kendall's Tau
print(f"\nKendall's Tau Test Results:")
print(f"Tau: {kendall_corr}, p-value: {kendall_p_value}")

if kendall_p_value < 0.05:
    print("Ada asosiasi signifikan antara ukuran file dan kategori.")
else:
    print("Tidak ada asosiasi signifikan antara ukuran file dan kategori.")

# Uji Chi-Squared: Uji independensi antara kategori dan format file
contingency_table = pd.crosstab(file_df['Category'], file_df['File Format'])
chi2, p, dof, expected = chi2_contingency(contingency_table)

print(f"\nChi-Squared Test Results:")
print(f"Chi-Squared: {chi2}, p-value: {p}")

if p < 0.05:
    print("Ada asosiasi signifikan antara kategori dan format file.")
else:
    print("Tidak ada asosiasi signifikan antara kategori dan format file.")

# Mutual Information: Mengukur seberapa banyak informasi yang dibagikan antara dua variabel
# Memerlukan konversi kategori menjadi numerik untuk ukuran file
from sklearn.preprocessing import LabelEncoder

# Encode kategori dan format file
label_encoder = LabelEncoder()
file_df['Category Encoded'] = label_encoder.fit_transform(file_df['Category'])
file_df['File Format Encoded'] = label_encoder.fit_transform(file_df['File Format'])

# Menghitung Mutual Information antara kategori dan ukuran file
X = file_df[['Category Encoded', 'File Format Encoded']]
y = file_df['File Size (bytes)']

mutual_info = mutual_info_classif(X, y)
print(f"\nMutual Information between 'Category' and 'File Size (bytes)': {mutual_info[0]}")

# Visualisasi untuk ANOVA dan Chi-Squared
# ANOVA Visualisasi: Distribusi ukuran file antar kategori
sns.boxplot(x='Category', y='File Size (bytes)', data=file_df)
plt.title('ANOVA: Ukuran File Antar Kategori')
plt.xticks(rotation=45)
plt.show()

# Chi-Squared Visualisasi: Tabel kontingensi antara kategori dan format file
sns.heatmap(contingency_table, annot=True, cmap="Blues", fmt="d")
plt.title('Chi-Squared: Kategori vs Format File')
plt.show()

"""### **`Memvalidasi data`**

**`Ukuran Data`**
"""

import os
import pandas as pd

# Path dataset
dataset_path = '/root/.cache/kagglehub/datasets/fthnaja/kain-ulos/versions/3'

# Fungsi untuk mendapatkan deskripsi dataset dan validasi ukuran file
def validate_file_sizes(dataset_path, min_size=0, max_size=float('inf')):
    file_details = []

    # Menelusuri seluruh file dan folder
    for root, dirs, files in os.walk(dataset_path):
        for file in files:
            if not file.startswith('.') and file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp')):
                # Menentukan kategori berdasarkan nama folder
                category = os.path.basename(root)
                file_format = file.split('.')[-1]  # Mendapatkan ekstensi file
                file_path = os.path.join(root, file)  # Mendapatkan path file lengkap

                # Mendapatkan karakteristik file: ukuran dan tanggal terakhir dimodifikasi
                file_size = os.path.getsize(file_path)  # Ukuran file dalam byte
                last_modified = os.path.getmtime(file_path)  # Timestamp terakhir dimodifikasi

                # Memvalidasi ukuran file apakah sesuai dengan batasan yang diberikan
                size_status = "Valid"
                if file_size < min_size:
                    size_status = f"Too small ({file_size} bytes)"
                elif file_size > max_size:
                    size_status = f"Too large ({file_size} bytes)"

                # Menyimpan informasi dalam bentuk dictionary
                file_details.append({
                    'Category': category,
                    'File Name': file,
                    'File Format': file_format,
                    'File Size (bytes)': file_size,
                    'Size Status': size_status,
                    'Last Modified': last_modified
                })

    # Mengubah hasil ke dalam bentuk DataFrame
    file_df = pd.DataFrame(file_details)
    return file_df

# Menentukan batasan ukuran file (misalnya: min_size = 1000 bytes, max_size = 5 MB)
min_size = 1000  # minimal 1 KB
max_size = 5 * 1024 * 1024  # maksimal 5 MB

# Mendapatkan deskripsi dataset dan validasi ukuran file
file_df = validate_file_sizes(dataset_path, min_size, max_size)

# Menampilkan data yang valid (ukuran file sesuai) dan yang tidak valid
valid_files = file_df[file_df['Size Status'] == 'Valid']
invalid_files = file_df[file_df['Size Status'] != 'Valid']

# Menampilkan hasil
print("Valid Files:")
print(valid_files[['Category', 'File Name', 'File Size (bytes)', 'Size Status']])

print("\nInvalid Files:")
print(invalid_files[['Category', 'File Name', 'File Size (bytes)', 'Size Status']])

"""**`Deskripsi Statistical Atribut`**"""

import os
import pandas as pd

# Path dataset
dataset_path = '/root/.cache/kagglehub/datasets/fthnaja/kain-ulos/versions/3'

# Fungsi untuk mendapatkan deskripsi dataset dan validasi ukuran file
def get_file_statistics(dataset_path):
    file_details = []

    # Menelusuri seluruh file dan folder
    for root, dirs, files in os.walk(dataset_path):
        for file in files:
            if not file.startswith('.') and file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp')):
                # Menentukan kategori berdasarkan nama folder
                category = os.path.basename(root)
                file_format = file.split('.')[-1]  # Mendapatkan ekstensi file
                file_path = os.path.join(root, file)  # Mendapatkan path file lengkap

                # Mendapatkan karakteristik file: ukuran dan tanggal terakhir dimodifikasi
                file_size = os.path.getsize(file_path)  # Ukuran file dalam byte
                last_modified = os.path.getmtime(file_path)  # Timestamp terakhir dimodifikasi

                # Menyimpan informasi dalam bentuk dictionary
                file_details.append({
                    'Category': category,
                    'File Name': file,
                    'File Format': file_format,
                    'File Size (bytes)': file_size,
                    'Last Modified': last_modified
                })

    # Mengubah hasil ke dalam bentuk DataFrame
    file_df = pd.DataFrame(file_details)
    return file_df

# Mendapatkan deskripsi dataset dan validasi ukuran file
file_df = get_file_statistics(dataset_path)

# Menampilkan deskripsi statistik dari atribut numerik
print("Deskripsi Statistik Atribut (Ukuran File dalam Byte):")
print(file_df['File Size (bytes)'].describe())

# Menampilkan statistik deskriptif kategori (kategori gambar)
print("\nFrekuensi Kategori:")
print(file_df['Category'].value_counts())

# Menampilkan statistik deskriptif terkait format file (ekstensi file)
print("\nFrekuensi Format File:")
print(file_df['File Format'].value_counts())

"""**`Relasi Antar Atribut`**"""

import os
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Path dataset
dataset_path = '/root/.cache/kagglehub/datasets/fthnaja/kain-ulos/versions/3'

# Fungsi untuk mendapatkan statistik file
def get_file_statistics(dataset_path):
    file_details = []

    # Menelusuri seluruh file dan folder
    for root, dirs, files in os.walk(dataset_path):
        for file in files:
            if not file.startswith('.') and file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp')):
                # Menentukan kategori berdasarkan nama folder
                category = os.path.basename(root)
                file_format = file.split('.')[-1]  # Mendapatkan ekstensi file
                file_path = os.path.join(root, file)  # Mendapatkan path file lengkap

                # Mendapatkan karakteristik file: ukuran dan tanggal terakhir dimodifikasi
                file_size = os.path.getsize(file_path)  # Ukuran file dalam byte
                last_modified = os.path.getmtime(file_path)  # Timestamp terakhir dimodifikasi

                # Menyimpan informasi dalam bentuk dictionary
                file_details.append({
                    'Category': category,
                    'File Name': file,
                    'File Format': file_format,
                    'File Size (bytes)': file_size,
                    'Last Modified': last_modified
                })

    # Mengubah hasil ke dalam bentuk DataFrame
    file_df = pd.DataFrame(file_details)
    return file_df

# Mendapatkan deskripsi dataset dan validasi ukuran file
file_df = get_file_statistics(dataset_path)

# Menampilkan korelasi antar atribut numerik
print("\nKorelasi antar atribut numerik (hanya ukuran file dan waktu modifikasi):")
numeric_columns = ['File Size (bytes)', 'Last Modified']
correlation_matrix = file_df[numeric_columns].corr()

# Menampilkan heatmap korelasi
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title('Heatmap Korelasi Antar Atribut Numerik')
plt.show()

# Menampilkan relasi antara kategori dan format file menggunakan Cross Tabulation
print("\nCross-Tabulation antara Kategori dan Format File:")
category_format_ct = pd.crosstab(file_df['Category'], file_df['File Format'])
print(category_format_ct)

# Visualisasi Cross-Tabulation menggunakan Heatmap
sns.heatmap(category_format_ct, annot=True, cmap="Blues", fmt="d")
plt.title('Cross-Tabulation antara Kategori dan Format File')
plt.ylabel('Category')
plt.xlabel('File Format')
plt.show()

"""**`Visualisasi Data`**"""

import os
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Path dataset
dataset_path = '/root/.cache/kagglehub/datasets/fthnaja/kain-ulos/versions/3'

# Fungsi untuk mendapatkan statistik file
def get_file_statistics(dataset_path):
    file_details = []

    # Menelusuri seluruh file dan folder
    for root, dirs, files in os.walk(dataset_path):
        for file in files:
            if not file.startswith('.') and file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp')):
                # Menentukan kategori berdasarkan nama folder
                category = os.path.basename(root)
                file_format = file.split('.')[-1]  # Mendapatkan ekstensi file
                file_path = os.path.join(root, file)  # Mendapatkan path file lengkap

                # Mendapatkan karakteristik file: ukuran dan tanggal terakhir dimodifikasi
                file_size = os.path.getsize(file_path)  # Ukuran file dalam byte
                last_modified = os.path.getmtime(file_path)  # Timestamp terakhir dimodifikasi

                # Menyimpan informasi dalam bentuk dictionary
                file_details.append({
                    'Category': category,
                    'File Name': file,
                    'File Format': file_format,
                    'File Size (bytes)': file_size,
                    'Last Modified': last_modified
                })

    # Mengubah hasil ke dalam bentuk DataFrame
    file_df = pd.DataFrame(file_details)
    return file_df

# Mendapatkan deskripsi dataset dan validasi ukuran file
file_df = get_file_statistics(dataset_path)

# 1. Visualisasi distribusi ukuran file menggunakan Histogram
plt.figure(figsize=(10, 6))
sns.histplot(file_df['File Size (bytes)'], kde=True, color='skyblue')
plt.title('Distribusi Ukuran File')
plt.xlabel('Ukuran File (Bytes)')
plt.ylabel('Frekuensi')
plt.grid(True)
plt.show()

# 2. Boxplot untuk distribusi ukuran file
plt.figure(figsize=(8, 6))
sns.boxplot(x=file_df['File Size (bytes)'], color='lightgreen')
plt.title('Boxplot Ukuran File')
plt.xlabel('Ukuran File (Bytes)')
plt.show()

# 3. Visualisasi kategori file berdasarkan kategori menggunakan Barplot
plt.figure(figsize=(12, 6))
category_counts = file_df['Category'].value_counts()
sns.barplot(x=category_counts.index, y=category_counts.values, palette='muted')
plt.title('Jumlah File per Kategori')
plt.xlabel('Kategori')
plt.ylabel('Jumlah File')
plt.xticks(rotation=45)
plt.show()

# 4. Pie chart untuk distribusi format file
plt.figure(figsize=(8, 8))
file_format_counts = file_df['File Format'].value_counts()
plt.pie(file_format_counts, labels=file_format_counts.index, autopct='%1.1f%%', colors=sns.color_palette("Set3", len(file_format_counts)))
plt.title('Distribusi Format File')
plt.show()

"""# **`Data Preparation`**

### **`Memilih dan Memilah Data`**
"""

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Path dataset
dataset_path = '/root/.cache/kagglehub/datasets/fthnaja/kain-ulos/versions/3'

# Direktori untuk kategori gambar
categories = ['Tumtuman', 'Pinuncaan', 'Ragi Hotang', 'Sibolang', 'Sadum', 'Ragi Hidup']

def prepare_data(dataset_path, categories):
    data = []
    labels = []

    # Iterasi untuk folder 'Train' dan 'Test'
    for split in ['Train', 'Test']:
        split_path = os.path.join(dataset_path, split)

        for category in categories:
            category_path = os.path.join(split_path, category)

            # Pastikan folder kategori ada
            if os.path.exists(category_path):
                # Memilih gambar dari folder kategori
                for filename in os.listdir(category_path):
                    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
                        img_path = os.path.join(category_path, filename)
                        try:
                            img = tf.keras.preprocessing.image.load_img(img_path, target_size=(128, 128))  # Resize gambar
                            img_array = tf.keras.preprocessing.image.img_to_array(img)
                            data.append(img_array)
                            labels.append(category)
                        except Exception as e:
                            print(f"Error loading image {img_path}: {e}")
            else:
                print(f"Folder not found: {category_path}")

    # Konversi data dan labels ke array numpy
    data = np.array(data)
    labels = np.array(labels)

    return data, labels

# Menyiapkan data
data, labels = prepare_data(dataset_path, categories)

# Normalisasi pixel gambar ke rentang [0, 1]
data = data / 255.0

# Membagi data menjadi training dan testing set
X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)

# Menggunakan OneHotEncoder untuk mengubah label menjadi format numerik
encoder = LabelEncoder()
y_train = encoder.fit_transform(y_train)
y_test = encoder.transform(y_test)

# Menyusun data augmentation untuk meningkatkan variasi data pelatihan
train_datagen = ImageDataGenerator(
    rescale=1./255,  # Normalisasi
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

validation_datagen = ImageDataGenerator(rescale=1./255)

# Menggunakan ImageDataGenerator untuk memproses data pelatihan
train_generator = train_datagen.flow(X_train, y_train, batch_size=32)
validation_generator = validation_datagen.flow(X_test, y_test, batch_size=32)

print("Data Preparation Completed")

"""### **`Membersihkan Data`**"""

import os
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Path dataset
dataset_path = '/root/.cache/kagglehub/datasets/fthnaja/kain-ulos/versions/3'

# Direktori untuk kategori gambar
categories = ['Tumtuman', 'Pinuncaan', 'Ragi Hotang', 'Sibolang', 'Sadum', 'Ragi Hidup']

def clean_data(dataset_path, categories, target_size=(128, 128)):
    data = []
    labels = []

    # Iterasi untuk folder 'Train' dan 'Test'
    for split in ['Train', 'Test']:
        split_path = os.path.join(dataset_path, split)

        for category in categories:
            category_path = os.path.join(split_path, category)

            # Pastikan folder kategori ada
            if os.path.exists(category_path):
                # Memilih gambar dari folder kategori
                for filename in os.listdir(category_path):
                    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
                        img_path = os.path.join(category_path, filename)
                        try:
                            # Memuat dan meresize gambar
                            img = tf.keras.preprocessing.image.load_img(img_path, target_size=target_size)
                            img_array = tf.keras.preprocessing.image.img_to_array(img)
                            data.append(img_array)
                            labels.append(category)
                        except Exception as e:
                            print(f"Error loading image {img_path}: {e}")
            else:
                print(f"Folder not found: {category_path}")

    # Konversi data dan labels ke array numpy
    data = np.array(data)
    labels = np.array(labels)

    return data, labels

# Menyiapkan data
data, labels = clean_data(dataset_path, categories)

# Normalisasi pixel gambar ke rentang [0, 1]
data = data / 255.0

# Membagi data menjadi training dan testing set
X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)

# Menggunakan OneHotEncoder untuk mengubah label menjadi format numerik
encoder = LabelEncoder()
y_train = encoder.fit_transform(y_train)
y_test = encoder.transform(y_test)

print(f"Data Cleaning Completed. Total images: {len(data)}")

"""### **`Mengkonstruksi Data `**"""

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Path dataset
dataset_path = '/root/.cache/kagglehub/datasets/fthnaja/kain-ulos/versions/3'

# Direktori untuk kategori gambar
categories = ['Tumtuman', 'Pinuncaan', 'Ragi Hotang', 'Sibolang', 'Sadum', 'Ragi Hidup']

def construct_data(dataset_path, categories, target_size=(128, 128)):
    data = []
    labels = []

    # Iterasi untuk folder 'Train' dan 'Test'
    for split in ['Train', 'Test']:
        split_path = os.path.join(dataset_path, split)

        for category in categories:
            category_path = os.path.join(split_path, category)

            if os.path.exists(category_path):
                # Memilih gambar dari folder kategori
                for filename in os.listdir(category_path):
                    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
                        img_path = os.path.join(category_path, filename)
                        img = tf.keras.preprocessing.image.load_img(img_path, target_size=target_size)  # Resize gambar
                        img_array = tf.keras.preprocessing.image.img_to_array(img)
                        data.append(img_array)
                        labels.append(category)

    # Konversi data dan labels ke array numpy
    data = np.array(data)
    labels = np.array(labels)

    return data, labels

# Menyiapkan data
data, labels = construct_data(dataset_path, categories)

# Normalisasi pixel gambar ke rentang [0, 1]
data = data / 255.0

# Membagi data menjadi training dan testing set
X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)

# Menggunakan LabelEncoder untuk mengubah label menjadi format numerik
encoder = LabelEncoder()
y_train = encoder.fit_transform(y_train)
y_test = encoder.transform(y_test)

# Menggunakan ImageDataGenerator untuk augmentasi dan validasi data
train_datagen = ImageDataGenerator(
    rescale=1./255,  # Normalisasi
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

validation_datagen = ImageDataGenerator(rescale=1./255)

# Menyiapkan data generator untuk training dan validation
train_generator = train_datagen.flow(X_train, y_train, batch_size=32)
validation_generator = validation_datagen.flow(X_test, y_test, batch_size=32)

print(f"Data Construction Completed. Total images: {len(data)}")

"""### **`Integrasi Data`**"""

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Path dataset
dataset_path = '/root/.cache/kagglehub/datasets/fthnaja/kain-ulos/versions/3'

# Direktori untuk kategori gambar
categories = ['Tumtuman', 'Pinuncaan', 'Ragi Hotang', 'Sibolang', 'Sadum', 'Ragi Hidup']

def integrate_data(dataset_path, categories, target_size=(128, 128)):
    data = []
    labels = []

    # Iterasi untuk folder 'Train' dan 'Test'
    for split in ['Train', 'Test']:
        split_path = os.path.join(dataset_path, split)

        for category in categories:
            category_path = os.path.join(split_path, category)

            if os.path.exists(category_path):
                # Memilih gambar dari folder kategori
                for filename in os.listdir(category_path):
                    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
                        img_path = os.path.join(category_path, filename)
                        img = tf.keras.preprocessing.image.load_img(img_path, target_size=target_size)  # Resize gambar
                        img_array = tf.keras.preprocessing.image.img_to_array(img)
                        data.append(img_array)
                        labels.append(category)

    # Konversi data dan labels ke array numpy
    data = np.array(data)
    labels = np.array(labels)

    return data, labels

# Menyiapkan data
data, labels = integrate_data(dataset_path, categories)

# Normalisasi pixel gambar ke rentang [0, 1]
data = data / 255.0

# Membagi data menjadi training dan testing set
X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)

# Menggunakan LabelEncoder untuk mengubah label menjadi format numerik
encoder = LabelEncoder()
y_train = encoder.fit_transform(y_train)
y_test = encoder.transform(y_test)

# Menggunakan ImageDataGenerator untuk augmentasi dan validasi data
train_datagen = ImageDataGenerator(
    rescale=1./255,  # Normalisasi
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

validation_datagen = ImageDataGenerator(rescale=1./255)

# Menyiapkan data generator untuk training dan validation
train_generator = train_datagen.flow(X_train, y_train, batch_size=32)
validation_generator = validation_datagen.flow(X_test, y_test, batch_size=32)

print(f"Data Integration Completed. Total images: {len(data)}")

"""# **`Modeling`**

**`Membangun Model CNN`**
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models

dataset_path = '/root/.cache/kagglehub/datasets/fthnaja/kain-ulos/versions/3'
train_dir = os.path.join(dataset_path, 'Train')
test_dir = os.path.join(dataset_path, 'Test')

# Direktori untuk kategori gambar
categories = ['Tumtuman', 'Pinuncaan', 'Ragi Hotang', 'Sibolang', 'Sadum', 'Ragi Hidup']

# Data Augmentation untuk latih
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Menggunakan ImageDataGenerator untuk memuat gambar dari folder latih
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(150, 150),  # Ukuran gambar yang diinginkan
    batch_size=32,
    class_mode='categorical'  # Gunakan 'binary' jika dataset biner
)

# Definisikan model CNN
model = models.Sequential()

# Lapisan konvolusi pertama
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))
model.add(layers.MaxPooling2D((2, 2)))

# Lapisan konvolusi kedua
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))

# Lapisan konvolusi ketiga
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))

# Lapisan konvolusi keempat
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))

# Lapisan Flatten untuk mengubah data menjadi bentuk 1D sebelum masuk ke Fully Connected layer
model.add(layers.Flatten())

# Fully Connected layer
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dropout(0.5))  # Dropout untuk mencegah overfitting

# Output layer
model.add(layers.Dense(train_generator.num_classes, activation='softmax'))  # 'softmax' untuk multi-class, 'sigmoid' untuk binary

# Menampilkan ringkasan arsitektur model
model.summary()

print("Model Architecture Completed")

"""**`Melatih Model CNN`**"""

# Kompilasi model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',  # Gunakan 'binary_crossentropy' untuk dataset biner
              metrics=['accuracy'])

# Melatih model
history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    epochs=30  # Tentukan jumlah epoch yang diinginkan
)

# Menyimpan model
model.save('model_ulos.h5')

"""**`Menguji Model CNN`**"""

# Data untuk uji
test_datagen = ImageDataGenerator(rescale=1./255)

# Memuat data uji
test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='categorical',  # Gunakan 'binary' jika dataset biner
    shuffle=False  # Penting untuk tidak mengacak urutan data pada pengujian
)

# Melakukan prediksi pada data uji
predictions = model.predict(test_generator, steps=test_generator.samples // test_generator.batch_size, verbose=1)
predicted_classes = predictions.argmax(axis=1)  # Prediksi kelas dengan nilai tertinggi
true_classes = test_generator.classes  # Kelas yang sebenarnya

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import numpy as np

# Menghitung confusion matrix
cm = confusion_matrix(true_classes, predicted_classes)

# Menghitung TP, TN, FP, FN untuk setiap kelas
tp = np.diag(cm)  # Diagonal adalah True Positives
fp = cm.sum(axis=0) - tp  # Kolom sum dikurangi TP untuk FP
fn = cm.sum(axis=1) - tp  # Baris sum dikurangi TP untuk FN
tn = cm.sum() - (fp + fn + tp)  # Total sum dikurangi TP, FP, FN untuk TN

# Visualisasi Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())
plt.title("Confusion Matrix")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

# Visualisasi TP, TN, FP, FN untuk setiap kelas
labels = test_generator.class_indices.keys()  # Nama-nama kelas
bar_width = 0.2
index = np.arange(len(labels))



# Menghitung confusion matrix
cm = confusion_matrix(true_classes, predicted_classes)

# Menghitung TP, TN, FP, FN untuk setiap kelas
tp = np.diag(cm)  # Diagonal adalah True Positives
fp = cm.sum(axis=0) - tp  # Kolom sum dikurangi TP untuk FP
fn = cm.sum(axis=1) - tp  # Baris sum dikurangi TP untuk FN
tn = cm.sum() - (fp + fn + tp)  # Total sum dikurangi TP, FP, FN untuk TN

# Membuat matriks TP, TN, FP, FN
metrics_matrix = np.array([tp, tn, fp, fn]).T  # Transpose agar tiap baris mewakili kelas
metrics_labels = ["True Positive", "True Negative", "False Positive", "False Negative"]

# Membuat heatmap untuk TP, TN, FP, FN per kelas
plt.figure(figsize=(10, 6))
sns.heatmap(metrics_matrix, annot=True, fmt='d', cmap="coolwarm", xticklabels=metrics_labels, yticklabels=test_generator.class_indices.keys())

# Menambahkan judul dan label
plt.title("TP, TN, FP, FN per Class")
plt.xlabel("Metrics")
plt.ylabel("Classes")
plt.tight_layout()
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Menghitung accuracy
accuracy = accuracy_score(true_classes, predicted_classes)
print(f"Accuracy: {accuracy:.4f}")

# Precision, Recall, dan F1-Score untuk multi-class classification
precision = precision_score(true_classes, predicted_classes, average='macro')  # Average 'macro' untuk multi-class
recall = recall_score(true_classes, predicted_classes, average='macro')
f1 = f1_score(true_classes, predicted_classes, average='macro')

print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")

"""# **`Simpan dan Buat Model`**"""

# Menyimpan model
model.save('model_ulos.h5')

from google.colab import files
files.download('model_ulos.h5')

"""# **`Evaluasi Model CNN`**

### **`Evaluasi Performa Model`**

Berikut adalah hasil evaluasi performa model CNN yang telah dilakukan:
- Accuracy
  Model menghasilkan akurasi sebesar 93.27%, yang berarti ini menunjukkan akurasi yang sangat tinggi dan sebagian besar prediksi model adalah benar.
- Precision
  Nilai precision yang dihasilkan adalah 93.92%, yang menunjukkan bahwa hampir semua prediksi positif yang dihasilkan oleh model adalah benar (low false positives).
- Recall:
  Model menghasilkan nilai recall sebesar 93.31%, yang menunjukkan bahwa model dapat melakukan deteksi pada sebagian besar data positif dengan sangat baik. Ini juga menunjukkan bahwa model yang digunakan tidak melewatkan banyak contoh positif (low false negatives).
- F1-Score:
  Dengan nilai F1-Score sebesar 93.30%, menunjukkan bahwa model yang digunakan memiliki keseimbangan yang baik antara precision dan recall.

Jadi dapat disimpulkan, bahwa model CNN yang digunakan sudah sangat baik.

### **`Evaluasi Proses`**

Meskipun model CNN yang digunakan sudah sangat baik, masih terdapat beberapa hal pada proses yang bisa diperbaiki, seperti:
- Penanganan overfitting yang dapat dilihat dari perbedaan besar antara akurasi pelatihan dan akurasi pengujian. Model terlalu menyesuaikan dengan data latih dibandingkan dengan data uji.
- Model hanya dievaluasi pada data uji, dan tidak diuji pada set data yang lain, sehingga memungkinkan model tidak tergeneralisasi dengan baik.
"""